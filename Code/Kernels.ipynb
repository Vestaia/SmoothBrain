{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models and Basis Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import io\n",
    "import pandas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "#Predictors\n",
    "class linear(nn.Module):\n",
    "    def __init__(self, feature_dims):\n",
    "        super().__init__()\n",
    "        self.w = torch.nn.Parameter(torch.rand((feature_dims,)))\n",
    "        self.b = torch.nn.Parameter(torch.rand((1,)))\n",
    "    def forward(self, x):\n",
    "        return torch.sum(self.w * x, dim=1) + self.b\n",
    "        \n",
    "\n",
    "class logistic(nn.Module):\n",
    "    def __init__(self, feature_dims):\n",
    "        super().__init__()\n",
    "        self.w = torch.nn.Parameter(torch.rand((feature_dims,)))\n",
    "        self.b = torch.nn.Parameter(torch.rand((1,)))\n",
    "    def forward(self, x):\n",
    "        y = torch.sum(self.w * x, dim=1) + self.b\n",
    "        return 2. / (1 + torch.exp(-y)) - 1\n",
    "\n",
    "class multi_linear(nn.Module):\n",
    "    def __init__(self, feature_dims):\n",
    "        super().__init__()\n",
    "        self.conv = torch.nn.Conv1d(1, 1, 3, padding=1)\n",
    "        self.conv2 = torch.nn.Conv1d(1, 1, 3, padding=1)\n",
    "        self.conv3 = torch.nn.Conv1d(1, 1, 3, padding=1)\n",
    "        self.conv4 = torch.nn.Conv1d(1, 1, 3, padding=1)\n",
    "        self.fc1 = torch.nn.Linear(feature_dims, 5)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "#Loss functions\n",
    "class loss_wrapper(torch.nn.Module):\n",
    "    def __init__(self, loss_func):\n",
    "        super().__init__()\n",
    "        self.loss_func = loss_func\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        return self.loss_func(x,y)\n",
    "\n",
    "#Classifier\n",
    "def hinge_loss(x, y):\n",
    "    loss = 1 - x * y\n",
    "    return torch.mean(torch.where(loss < 0, torch.zeros_like(loss), loss))\n",
    "\n",
    "def mispredict_loss(x,y):\n",
    "    loss = - x * y\n",
    "    return torch.mean(torch.where(loss < 0, torch.zeros_like(loss), loss))\n",
    "\n",
    "def incorrect(x,y):\n",
    "    return torch.sum(x*y < 0) / len(x)\n",
    "\n",
    "#Regression\n",
    "def MSE_loss(x, y):\n",
    "    loss = torch.pow(y - x, 2)\n",
    "    return torch.mean(loss)\n",
    "\n",
    "def MAE_loss(x, y):\n",
    "    return torch.mean(torch.abs(y - x))\n",
    "\n",
    "\n",
    "def l2_regularizer(model, loss, strength):\n",
    "    def reg_loss(x, y):\n",
    "        return loss(x,y) + strength * torch.sum(torch.pow(torch.cat([param for param in model.parameters()]),2))\n",
    "    return reg_loss\n",
    "\n",
    "def l1_regularizer(model, loss, strength):\n",
    "    def reg_loss(x, y):\n",
    "        return loss(x,y) + strength * torch.sum(torch.abs(torch.cat([param for param in model.parameters()])))\n",
    "    return reg_loss\n",
    "\n",
    "def rbf(x, z, gamma):\n",
    "    return torch.exp(- gamma * torch.sum(torch.pow(x-z,2), dim=-1))\n",
    "\n",
    "def kernel_mat(kernel, x, **kwargs):\n",
    "    return kernel(x[:,None,:] , x[None,:,:], **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(net, optimizer, loss_func, X, y, batch_size=50, device='cpu'):\n",
    "\n",
    "    torch.backends.cudnn.fastest = True\n",
    "    calc = loss_func\n",
    "    net.train()\n",
    "    totalloss=0\n",
    "    indicies = torch.randperm(len(X))\n",
    "    batches = int(np.floor(len(X)/batch_size))\n",
    "    for batch in range(batches):\n",
    "        optimizer.zero_grad()\n",
    "        tx = X[indicies[batch*batch_size:(batch+1)*batch_size]].to(device)\n",
    "        ty = y[indicies[batch*batch_size:(batch+1)*batch_size]].to(device)\n",
    "        pred = net(tx)\n",
    "        loss = calc(pred, ty)\n",
    "        totalloss += loss.detach()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    loss = totalloss/batches\n",
    "    return loss.item()\n",
    "\n",
    "def test(net, loss_func, X, y, batch_size=50, device='cpu'):\n",
    "\n",
    "    calc = loss_func\n",
    "    net.eval()\n",
    "    totalloss=0\n",
    "    batches = int(np.floor(len(X)/batch_size))\n",
    "    for batch in range(batches):\n",
    "        tx = X[batch*batch_size:(batch+1)*batch_size].to(device)\n",
    "        ty = y[batch*batch_size:(batch+1)*batch_size].to(device)\n",
    "        pred = net(tx)\n",
    "        totalloss += calc(pred, ty).detach()\n",
    "    loss = totalloss/batches\n",
    "    return loss.item()\n",
    "\n",
    "from IPython import display\n",
    "def plot_train(trainloss, testloss):\n",
    "    plt.clf()\n",
    "    plt.plot(np.linspace(2,len(trainloss),len(trainloss)-1), trainloss[1:], label=\"train\")\n",
    "    plt.plot(np.linspace(2,len(testloss),len(testloss)-1), testloss[1:], label=\"test\")\n",
    "    plt.legend()\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss') \n",
    "    #display.display(plt.gcf())\n",
    "    plt.show()\n",
    "    #display.clear_output(wait=True)\n",
    "\n",
    "\n",
    "def train(model, optimizer, loss, X, y, validation_size, batch_size=100, epochs=1, plot=False, verbose=False, metric=mispredict_loss):\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    ind = np.random.permutation(len(y))\n",
    "    validationX = X[ind[:validation_size]]\n",
    "    validationy = y[ind[:validation_size]]\n",
    "    trainX = X[ind[validation_size:]]\n",
    "    trainy = y[ind[validation_size:]]\n",
    "    trainloss = []\n",
    "    validation_loss = []\n",
    "    plt.ion()\n",
    "    epoch = 0\n",
    "    for i in range(epochs):\n",
    "        trainloss.append(learn(model, optimizer, loss, trainX, trainy, batch_size=batch_size, device=device))\n",
    "        validation_loss.append(test(model, loss, validationX, validationy, batch_size=batch_size, device=device))\n",
    "        if(verbose):\n",
    "            print(\"Epoch:\", i+1)\n",
    "            print(\"Train loss:\", trainloss[i])\n",
    "            print(\"validation loss:\", validation_loss[i])\n",
    "        epoch += 1\n",
    "    if(plot):\n",
    "        plot_train(trainloss,validation_loss)\n",
    "    Err = test(model, loss_wrapper(metric), validationX, validationy, batch_size=batch_size, device=device)\n",
    "    print(\"Average Error (MAE): \" + str(Err))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "def separate_by_class(features, labels):\n",
    "    classes = torch.unique(labels)\n",
    "    separated = dict()\n",
    "    for c in classes:\n",
    "        separated[c] = features[labels == c]\n",
    "    return separated\n",
    "\n",
    "def gaussian(x, mean, std):\n",
    "    prob = (1 / (np.sqrt(2 * np.pi) * std)) * torch.exp(-((x-mean)**2 / (2 * std**2 )))\n",
    "    prob[:,std==0] = 1\n",
    "    return prob\n",
    "\n",
    "def summary(features, labels):\n",
    "    sum = dict()\n",
    "    separated = separate_by_class(features, labels)\n",
    "    for label in separated:\n",
    "        properties = dict()\n",
    "        properties['mean'] = torch.mean(separated[label], dim=0)\n",
    "        properties['std'] = torch.std(separated[label], dim=0)\n",
    "        properties['count'] = len(separated[label])\n",
    "        if (len(separated[label]) < 2):\n",
    "            properties['std'] = -1\n",
    "        sum[label] = properties\n",
    "    return sum\n",
    "\n",
    "def class_probability(sum, x):\n",
    "    total_count = 0\n",
    "    prob = torch.empty((len(x),len(sum)))\n",
    "    labels = []\n",
    "    for i, label in enumerate(sum):\n",
    "        mean = sum[label]['mean']\n",
    "        std = sum[label]['std']\n",
    "        count = sum[label]['count']\n",
    "        prob[:,i] = count * torch.prod(gaussian(x, mean, std),dim=1)\n",
    "        total_count += count\n",
    "        labels.append(label)\n",
    "    return prob / total_count, labels\n",
    "\n",
    "def discrete_class_probability(features, labels, x, smoothing=0):\n",
    "    separated = separate_by_class(features,labels)\n",
    "    prob = torch.empty((len(x),len(separated)))\n",
    "    lab = []\n",
    "    for i, label in enumerate(separated):\n",
    "        exist = separated[label] > 0\n",
    "        p = (x>0) * (torch.sum(exist,dim=0)+smoothing)/(len(exist)+smoothing*features.shape[-1])\n",
    "        p[x == 0] = 1\n",
    "        prob[:,i] = torch.prod(p, dim = 1)\n",
    "        lab.append(label)\n",
    "    return prob, lab\n",
    "    \n",
    "\n",
    "def predict(features, labels, x, discrete=False, smoothing=0):\n",
    "    if (discrete):\n",
    "        prob, label = discrete_class_probability(features, labels, x, smoothing) \n",
    "    else:\n",
    "        prob, label = class_probability(summary(features,labels), x) \n",
    "    return torch.argmax(prob,dim=1), label\n",
    "\n",
    "#Train validation split with random selection\n",
    "#Input format:\n",
    "#Two tensors with dimension [samples,feature_group1,feature_group2,...]\n",
    "def cross_val_split(features, labels, validation_size):\n",
    "    rand_ind = np.random.permutation(len(features))\n",
    "    training_features = features[rand_ind[validation_size:]]\n",
    "    training_labels = labels[rand_ind[validation_size:]]\n",
    "    validation_features = features[rand_ind[:validation_size]]\n",
    "    validation_labels = labels[rand_ind[:validation_size]]    \n",
    "    return (training_features,training_labels,validation_features,validation_labels)\n",
    "\n",
    "#Normalize by scaling each column linearly to range 0-1\n",
    "def normalize(x):\n",
    "    norm = (x - torch.min(x,dim=0)[0])/(torch.max(x,dim=0)[0] - torch.min(x,dim=0)[0])\n",
    "    norm[torch.isnan(norm)] = 0\n",
    "    return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
